'''
Created by Austin Abeyta on 6/1/2018

Run PreProcess.py before running NeuralNetwork

Google App Store Malware Classifier
    Classifies google play store applications as safe or malicious. The data is imported from
    Nuray Baltaci's paper; A COMPARISON OF CLASSIFICATION ALGORITHMS FOR MOBILE
    MALWARE DETECTION: MARKET METADATA AS INPUT SOURCE. The data set consists of permissions
    requested by the application as well as metadata.

'''
import tensorflow as tf
import PreProcess



LOGDIR = "/tmp/mnist_tutorial/"

#import training and testing array from Preprocess.py
testData = PreProcess.testData
trainingData =  PreProcess.trainingData
trainingLabels = PreProcess.trainingLabels
testLabels = PreProcess.testLabels

#Set the number of training samples and testing samples
N_SAMPELS = len(trainingData)
N_TEST_SAMPELS = len(testData)

#After un usable features are deleted count the number of features for in
NUMBER_OF_FEATURES =  len(trainingData[1,:])
print(NUMBER_OF_FEATURES)

# Hyperparamters
n_input = NUMBER_OF_FEATURES
n_hidden = 35
n_output = 2
learning_rate = 0.01
epochs = 5000

#How many training steps before cost is displayed
display_step = 100
#How many steps between each graph point is recorded
graphing_step = 25

#Layer one
with tf.name_scope("Layer1"):
    # Placeholders
    X = tf.placeholder(tf.float32)
    Y = tf.placeholder(tf.float32)

    # Weights
    W1 = tf.Variable(tf.random_uniform([n_input, n_hidden], -0.3, .3))
    # Bias
    b1 = tf.Variable(tf.zeros([n_hidden]))

    L2 = tf.sigmoid(tf.matmul(X, W1) + b1)

#Layer two
with tf.name_scope("Layer2"):
    W2 = tf.Variable(tf.random_uniform([n_hidden, n_output], -0.3, .3))
    b2 = tf.Variable(tf.zeros([n_output]))
    hy = tf.sigmoid(tf.matmul(L2, W2) + b2)


#cost formula
cost = tf.reduce_sum(tf.pow(hy-Y, 2))/(2*N_SAMPELS)




#summary
tf.summary.histogram("Weights1" , W1)
tf.summary.histogram("Bias1" , b1)
tf.summary.histogram("Weights2", W2)
tf.summary.histogram("Bias2 ", b2)
tf.summary.scalar("Cost", cost)

with tf.name_scope("Acuraccy"):
    answer = tf.equal(tf.floor(hy + 0.1), Y)
    accuracy = tf.reduce_mean(tf.cast(answer, "float"))
    tf.summary.scalar("accuracy", accuracy)

summ = tf.summary.merge_all()
writer =  tf.summary.FileWriter(LOGDIR)

#training step
with tf.name_scope("Train"):
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

init = tf.global_variables_initializer()


with tf.Session() as sess:
    sess.run(init)
    writer.add_graph(sess.graph)

    #training loop
    for step in range(epochs):
        if step == 1000:
            learning_rate = 0.001
        _, c = sess.run([optimizer, cost], feed_dict = {X: trainingData, Y: trainingLabels})

        #graph summary
        if step % graphing_step == 0:
            s = sess.run(summ, feed_dict={X: trainingData, Y: trainingLabels})

            writer.add_summary(s, step)
        if step % display_step == 0:
            print("Cost", c)
        writer.add_summary(s)

    #print out final accuracy
    answer = tf.equal(tf.floor(hy + 0.1), Y)
    accuracy = tf.reduce_mean(tf.cast(answer, "float"))

    print(sess.run([hy], feed_dict = {X: testData, Y: testLabels}))
    print("Accuracy", accuracy.eval({X: testData, Y: testLabels}))




print('Run `tensorboard --logdir=%s --host=localhost` to see the results.' % LOGDIR)